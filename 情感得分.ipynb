{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import time\n",
    "import csv\n",
    "# 停词表\n",
    "import requests\n",
    "import json\n",
    "import plotly.graph_objs as go\n",
    "import plotly\n",
    "import seaborn as sns\n",
    "from IPython.core.interactiveshell import InteractiveShell as IS\n",
    "IS.ast_node_interactivity = 'all'\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 情感词典构建,感叹号两倍，分句"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词，返回List\n",
    "jieba.load_userdict('70000-dict.txt')\n",
    "def read_lines(filename):\n",
    "    fp = open(filename, 'r',encoding='utf-8')\n",
    "    lines = []\n",
    "    for line in fp.readlines():\n",
    "        line = line.strip()\n",
    "        line = line.encode('utf-8').decode(\"utf-8\")\n",
    "        lines.append(line)\n",
    "    fp.close()\n",
    "    return lines\n",
    "def segmentation(sentence):\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    seg_result = []\n",
    "    for w in seg_list:\n",
    "        seg_result.append(w)\n",
    "    return seg_result\n",
    "# 去除停用词\n",
    "def del_stopwords(seg_sent):\n",
    "    stopwords = read_lines(\"Sentiment_dict/emotion_dict/stop_words.txt\")  # 读取停用词表\n",
    "    new_sent = []  # 去除停用词后的句子\n",
    "    for word in seg_sent:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            new_sent.append(word)\n",
    "    return new_sent\n",
    "def match(word, sentiment_value):\n",
    "    if word in mostdict:\n",
    "        sentiment_value *= 2.0\n",
    "    elif word in verydict:\n",
    "        sentiment_value *= 1.75\n",
    "    elif word in moredict:\n",
    "        sentiment_value *= 1.5\n",
    "    elif word in ishdict:\n",
    "        sentiment_value *= 1.2\n",
    "    elif word in insufficientdict:\n",
    "        sentiment_value *= 0.5\n",
    "    elif word in inversedict:\n",
    "        #print \"inversedict\", word\n",
    "        sentiment_value *= -1\n",
    "    return sentiment_value\n",
    "\n",
    "# 句子切分\n",
    "def cut_sentence(words):\n",
    "    words = words.encode('utf-8').decode('utf-8')\n",
    "    start = 0\n",
    "    i = 0\n",
    "    token = 'meaningless'\n",
    "    sents = []\n",
    "    punt_list = ',.!?;~，。！？；～… '.encode('utf-8').decode('utf-8')\n",
    "    #print \"punc_list\", punt_list\n",
    "    for word in words:\n",
    "        #print \"word\", word\n",
    "        if word not in punt_list:  # 如果不是标点符号\n",
    "            #print \"word1\", word\n",
    "            i += 1\n",
    "            token = list(words[start:i + 2]).pop()\n",
    "            #print \"token:\", token\n",
    "        elif word in punt_list and token in punt_list:  # 处理省略号\n",
    "            #print \"word2\", word\n",
    "            i += 1\n",
    "            token = list(words[start:i + 2]).pop()\n",
    "            #print \"token:\", token\n",
    "        else:\n",
    "            #print \"word3\", word\n",
    "            sents.append(words[start:i + 1])  # 断句\n",
    "            start = i + 1\n",
    "            i += 1\n",
    "    if start < len(words):  # 处理最后的部分\n",
    "        sents.append(words[start:])\n",
    "    return sents[:-1]\n",
    "\n",
    "def single_score(sentence):\n",
    "    single_review_senti_score = []\n",
    "    cuted_review=cut_sentence(sentence)\n",
    "    for sent in cuted_review:\n",
    "        seg_sent = segmentation(sent)   # 分词\n",
    "        seg_sent = del_stopwords(seg_sent)[:]\n",
    "        i = 0    # 记录扫描到的词的位置\n",
    "        s = 0    # 记录上一个情感词的位置\n",
    "        emotion_value = 0    # 记录该分句中的积极情感得分\n",
    "\n",
    "        for word in seg_sent:   # 逐词分析\n",
    "            # word限定了三个条件，所以只是关键词\n",
    "            #print word\n",
    "            if word in emotion_keys:\n",
    "                a = emotion_dict.get(word)\n",
    "                for w in seg_sent[s:i]:\n",
    "                    a = match(w, a)\n",
    "                emotion_value+=a\n",
    "                #print \"poscount:\", poscount\n",
    "                s = i + 1  # 记录情感词的位置变化\n",
    "\n",
    "            #如果是感叹号，表示已经到本句句尾\n",
    "            elif word == \"！\".encode('utf-8').decode(\"utf-8\") or word == \"!\".encode('utf-8').decode('utf-8'):\n",
    "                # 有问题\n",
    "                emotion_value*=2\n",
    "                break\n",
    "            i += 1\n",
    "        #print \"poscount,negcount\", poscount, negcount\n",
    "        single_review_senti_score.append(emotion_value)   # 对得分做最后处理\n",
    "\n",
    "    result=np.array(single_review_senti_score).sum()\n",
    "    result = round(result, 1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取语料计算情感得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\RUBYEE~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.747 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "# 情感词典构建\n",
    "\n",
    "# 分词，返回List\n",
    "jieba.load_userdict('70000-dict.txt')\n",
    "def read_lines(filename):\n",
    "    fp = open(filename, 'r',encoding='utf-8')\n",
    "    lines = []\n",
    "    for line in fp.readlines():\n",
    "        line = line.strip()\n",
    "        line = line.encode('utf-8').decode(\"utf-8\")\n",
    "        lines.append(line)\n",
    "    fp.close()\n",
    "    return lines\n",
    "## 程度词和否定词文件\n",
    "mostdict = read_lines('Sentiment_dict/degree_dict/most.txt')   # 权值为2\n",
    "verydict = read_lines('Sentiment_dict/degree_dict/very.txt')   # 权值为1.5\n",
    "moredict = read_lines('Sentiment_dict/degree_dict/more.txt')   # 权值为1.25\n",
    "ishdict = read_lines('Sentiment_dict/degree_dict/ish.txt')   # 权值为0.5\n",
    "insufficientdict = read_lines('Sentiment_dict/degree_dict/insufficiently.txt')  # 权值为0.25\n",
    "inversedict = read_lines('Sentiment_dict/degree_dict/inverse.txt')  # 权值为-1\n",
    "def segmentation(sentence):\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    seg_result = []\n",
    "    for w in seg_list:\n",
    "        seg_result.append(w)\n",
    "    return seg_result\n",
    "# 去除停用词\n",
    "def del_stopwords(seg_sent):\n",
    "    stopwords = read_lines(\"Sentiment_dict/emotion_dict/stop_words.txt\")  # 读取停用词表\n",
    "    new_sent = []  # 去除停用词后的句子\n",
    "    for word in seg_sent:\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            new_sent.append(word)\n",
    "    return new_sent\n",
    "def match(word, sentiment_value):\n",
    "    if word in mostdict:\n",
    "        sentiment_value *= 2.0\n",
    "    elif word in verydict:\n",
    "        sentiment_value *= 1.75\n",
    "    elif word in moredict:\n",
    "        sentiment_value *= 1.5\n",
    "    elif word in ishdict:\n",
    "        sentiment_value *= 1.2\n",
    "    elif word in insufficientdict:\n",
    "        sentiment_value *= 0.5\n",
    "    elif word in inversedict:\n",
    "        #print \"inversedict\", word\n",
    "        sentiment_value *= -1\n",
    "    return sentiment_value\n",
    "\n",
    "\n",
    "def get_keys():\n",
    "    emotion=pd.read_csv(\"Sentiment_dict/emotion_dict/BosonNLP_sentiment_score.txt\",sep='\\s+',header=None)\n",
    "#     CON=emotion.iloc[:,0].isin([',','.','!','?',';','~','，','。','！','？','；','～','… ','[',']'])\n",
    "#     emotion=emotion[~CON]\n",
    "    \n",
    "    emotion_dict=dict(zip(emotion.iloc[:,0],emotion.iloc[:,1]))\n",
    "    emotion_keys=emotion.iloc[:,0].values.tolist()\n",
    "    return emotion_keys,emotion_dict\n",
    "\n",
    "emotion_keys,emotion_dict=get_keys()\n",
    "\n",
    "def single_score(sentence):\n",
    "    seg_sent = del_stopwords(segmentation(sentence))\n",
    "    \n",
    "    i = 0    # 记录扫描到的词的位置\n",
    "    s = 0    # 记录上一个情感词的位置\n",
    "    emotion_value =[]   # 记录该分句中的积极情感得分\n",
    "    \n",
    "    for word in seg_sent:   # 逐词分析\n",
    "        # word限定了三个条件，所以只是关键词\n",
    "        \n",
    "        if word in emotion_keys:\n",
    "#             print('情感词',word)\n",
    "            a = emotion_dict.get(word)\n",
    "#             print('初始分',a)\n",
    "            for w in seg_sent[s:i]:\n",
    "#                 print('权重',w)\n",
    "                a = match(w, a)\n",
    "#                 print('加权后',a)\n",
    "            # 记录该词的情感值 \n",
    "            emotion_value.append(a)\n",
    "            s = i + 1  # 记录情感词的位置变化\n",
    "#         if word == \"！\" or word == \"!\":\n",
    "#             try:\n",
    "#                 emotion_value[-1]*=2\n",
    "#             except:\n",
    "#                 print('error')\n",
    "        i += 1\n",
    "    result = round(sum(emotion_value), 1)\n",
    "    return result,len(emotion_value)\n",
    "\n",
    "def cal_score(original_name='缺判断缺评分',name='缺判断'):\n",
    "    # 输入original为读入文件，name为读出文件\n",
    "    import time\n",
    "    start_time=time.time()\n",
    "    \n",
    "    _ = open(\"D:\\\\IDEA\\\\情感分析论文\\\\论文数据\\\\\" + name + '.csv',\n",
    "             'w',\n",
    "             encoding='utf-8')\n",
    "    _.close()\n",
    "    print(name + '.csv已创建')\n",
    "\n",
    "    # 创建写入器\n",
    "    csvfile = open(\"D:\\\\IDEA\\\\情感分析论文\\\\论文数据\\\\\" + name + '.csv',\n",
    "                   'a+',\n",
    "                   newline='',\n",
    "                   encoding='utf-8')\n",
    "    # a+表示以追加模式写入，如果用w会覆盖掉原来的数据\n",
    "    csv_write = csv.writer(csvfile)\n",
    "\n",
    "    # 读取并写入\n",
    "    print(name + '.csv正在写入')\n",
    "    f = open('D:\\\\IDEA\\\\情感分析论文\\\\论文数据\\\\'+original_name+'.csv',\n",
    "             encoding='utf-8')\n",
    "    csv_read = csv.reader(f)\n",
    "    for i,line in enumerate(csv_read):\n",
    "        score,length=single_score(line[0])\n",
    "        print(i,score)\n",
    "        _ = csv_write.writerow([score,length])\n",
    "    csvfile.close()\n",
    "    f.close()\n",
    "    print(name + '.csv写入完成')\n",
    "    end_time=time.time()\n",
    "    print('%.8s'%(end_time-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试一些词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['《',\n",
       " '济南',\n",
       " '战',\n",
       " '“',\n",
       " '疫',\n",
       " '”',\n",
       " '》',\n",
       " '：',\n",
       " '今日',\n",
       " '报道',\n",
       " '了',\n",
       " '济南市',\n",
       " '三例',\n",
       " '治愈',\n",
       " '出院',\n",
       " '患者',\n",
       " '，',\n",
       " '祝福',\n",
       " '！',\n",
       " '加油',\n",
       " '！',\n",
       " '[',\n",
       " '加油',\n",
       " ']',\n",
       " '[',\n",
       " '加油',\n",
       " ']',\n",
       " '[',\n",
       " '加油',\n",
       " ']',\n",
       " ' ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(30.5, 29)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.37612173454"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words='《济南战“疫”》：今日报道了济南市三例治愈出院的患者，祝福他们！加油！[加油][加油][加油] '\n",
    "\n",
    "seg_sent=segmentation(words)   # 分\n",
    "sen=del_stopwords(seg_sent)\n",
    "# 分词\n",
    "sen\n",
    "# 得分\n",
    "single_score(words)\n",
    "emotion_keys,emotion_dict=get_keys()\n",
    "emotion_dict.get('2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['这两天',\n",
       " '看到',\n",
       " '太多',\n",
       " '黑暗面',\n",
       " '，',\n",
       " '红十字',\n",
       " '的不',\n",
       " '，',\n",
       " '一线',\n",
       " '人员',\n",
       " '可怜',\n",
       " '，',\n",
       " '疫情',\n",
       " '不断',\n",
       " '爆发',\n",
       " '，',\n",
       " '心',\n",
       " '太累',\n",
       " '了',\n",
       " '，',\n",
       " '听到',\n",
       " '这首歌',\n",
       " '，',\n",
       " '心灵',\n",
       " '得到',\n",
       " '了',\n",
       " '短暂',\n",
       " '慰籍',\n",
       " '，',\n",
       " '加油',\n",
       " '，',\n",
       " '武汉',\n",
       " '，',\n",
       " '正能量',\n",
       " '人在',\n",
       " '支撑']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_stopwords(segmentation('这两天看到太多黑暗面，红十字的不作为，一线人员的可怜，疫情的不断爆发，心太累了，听到这首歌，心灵得到了短暂的慰籍，加油，武汉，还是有正能量人在支撑我们'))\n",
    "emotion_dict.get('允悲')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
